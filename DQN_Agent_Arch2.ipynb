{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver\n",
    "import os\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "States_track = collections.defaultdict(dict)\n",
    "\n",
    "rewards_tracked = {}\n",
    "for i in range(0,5):\n",
    "    for j in range(0,24):\n",
    "        for k in range(0,7):\n",
    "            rewards_tracked[(i,j,k)]=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actions=[(0,0)]\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        if i!=j:\n",
    "            total_actions.append((i,j))\n",
    "\n",
    "total_state = [(i,j,k) for i in range(0,5) for j in range(0,24) for k in range(0,7)]\n",
    "\n",
    "for state in total_state:\n",
    "    Q_dict[state] = {}\n",
    "    for action in total_actions:\n",
    "        Q_dict[state][action] = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_action_indx(val):\n",
    "    for i,item in enumerate(total_actions):\n",
    "        if val == item:\n",
    "            return i\n",
    "    return 0\n",
    "\n",
    "def convert_into_str(arr):\n",
    "    text=\"\"\n",
    "    for val in arr:\n",
    "        text=text+\"-\"+str(int(val))\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[(0,0,0)][0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.85\n",
    "        self.learning_rate =  0.01    \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 32\n",
    "        self.epsilon = 1\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        input_shape=self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state, action):\n",
    "        # Write your code here:\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return action[random.randrange(self.action_size)]\n",
    "        else:\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            return total_actions[np.argmax(q_value[0])]\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state):\n",
    "        # Write your code here:\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "    \n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self,terminal_state):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards = [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state = mini_batch[i]\n",
    "                state_encod = env.state_encod_arch2(state,action)\n",
    "                \n",
    "                update_input[i] = state_encod\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch2(next_state,(0,0))\n",
    "                \n",
    "            #Predict the target from earlier model \n",
    "            target = self.model.predict(update_input)\n",
    "\n",
    "            #Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            for i in range(self.batch_size):\n",
    "                if terminal_state:\n",
    "                    target[i][find_action_indx(actions[i])] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][find_action_indx(actions[i])] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "                \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1125 18:10:20.578776  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1125 18:10:20.612588  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1125 18:10:20.616577  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1125 18:10:20.699354  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1125 18:10:20.718302  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1125 18:10:20.721298  4528 deprecation_wrapper.py:119] From C:\\Users\\debasish.b.sahoo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward -545.0, memory_length 236, epsilon 0.99999, time_taken 732.0\n",
      "model saved\n",
      "episode 10, reward -501.0, memory_length 2000, epsilon 0.9950025290678904, time_taken 725.0\n",
      "episode 20, reward -890.0, memory_length 2000, epsilon 0.9900399332508306, time_taken 724.0\n",
      "episode 30, reward -177.0, memory_length 2000, epsilon 0.9851020884836666, time_taken 725.0\n",
      "episode 40, reward -226.0, memory_length 2000, epsilon 0.9801888713200222, time_taken 733.0\n",
      "episode 50, reward -469.0, memory_length 2000, epsilon 0.9753001589292124, time_taken 725.0\n",
      "episode 60, reward -104.0, memory_length 2000, epsilon 0.9704358290931727, time_taken 726.0\n",
      "episode 70, reward -736.0, memory_length 2000, epsilon 0.965595760203404, time_taken 733.0\n",
      "episode 80, reward -885.0, memory_length 2000, epsilon 0.9607798312579316, time_taken 724.0\n",
      "episode 90, reward -459.0, memory_length 2000, epsilon 0.9559879218582816, time_taken 725.0\n",
      "episode 100, reward -423.0, memory_length 2000, epsilon 0.9512199122064691, time_taken 724.0\n",
      "episode 110, reward -321.0, memory_length 2000, epsilon 0.9464756831020044, time_taken 728.0\n",
      "episode 120, reward 17.0, memory_length 2000, epsilon 0.9417551159389129, time_taken 724.0\n",
      "episode 130, reward -479.0, memory_length 2000, epsilon 0.9370580927027697, time_taken 725.0\n",
      "episode 140, reward 523.0, memory_length 2000, epsilon 0.9323844959677493, time_taken 728.0\n",
      "episode 150, reward -299.0, memory_length 2000, epsilon 0.9277342088936896, time_taken 727.0\n",
      "episode 160, reward -112.0, memory_length 2000, epsilon 0.9231071152231719, time_taken 726.0\n",
      "episode 170, reward -133.0, memory_length 2000, epsilon 0.9185030992786134, time_taken 733.0\n",
      "episode 180, reward -419.0, memory_length 2000, epsilon 0.9139220459593755, time_taken 729.0\n",
      "episode 190, reward -6.0, memory_length 2000, epsilon 0.9093638407388868, time_taken 723.0\n",
      "episode 200, reward -337.0, memory_length 2000, epsilon 0.9048283696617792, time_taken 730.0\n",
      "episode 210, reward -425.0, memory_length 2000, epsilon 0.9003155193410398, time_taken 729.0\n",
      "episode 220, reward -151.0, memory_length 2000, epsilon 0.8958251769551753, time_taken 723.0\n",
      "episode 230, reward -732.0, memory_length 2000, epsilon 0.8913572302453923, time_taken 724.0\n",
      "episode 240, reward -290.0, memory_length 2000, epsilon 0.8869115675127903, time_taken 726.0\n",
      "episode 250, reward -277.0, memory_length 2000, epsilon 0.8824880776155697, time_taken 722.0\n",
      "episode 260, reward -216.0, memory_length 2000, epsilon 0.8780866499662522, time_taken 726.0\n",
      "episode 270, reward -241.0, memory_length 2000, epsilon 0.8737071745289176, time_taken 732.0\n",
      "episode 280, reward -308.0, memory_length 2000, epsilon 0.8693495418164519, time_taken 722.0\n",
      "episode 290, reward -256.0, memory_length 2000, epsilon 0.8650136428878102, time_taken 723.0\n",
      "episode 300, reward 587.0, memory_length 2000, epsilon 0.8606993693452936, time_taken 723.0\n",
      "episode 310, reward 207.0, memory_length 2000, epsilon 0.8564066133318388, time_taken 730.0\n",
      "episode 320, reward -134.0, memory_length 2000, epsilon 0.8521352675283217, time_taken 724.0\n",
      "episode 330, reward -648.0, memory_length 2000, epsilon 0.847885225150875, time_taken 724.0\n",
      "episode 340, reward -354.0, memory_length 2000, epsilon 0.8436563799482177, time_taken 731.0\n",
      "episode 350, reward -622.0, memory_length 2000, epsilon 0.8394486261989997, time_taken 733.0\n",
      "episode 360, reward -15.0, memory_length 2000, epsilon 0.8352618587091579, time_taken 727.0\n",
      "episode 370, reward -114.0, memory_length 2000, epsilon 0.8310959728092872, time_taken 731.0\n",
      "episode 380, reward -167.0, memory_length 2000, epsilon 0.8269508643520229, time_taken 731.0\n",
      "episode 390, reward 90.0, memory_length 2000, epsilon 0.8228264297094379, time_taken 724.0\n",
      "episode 400, reward -265.0, memory_length 2000, epsilon 0.8187225657704511, time_taken 726.0\n",
      "episode 410, reward 4.0, memory_length 2000, epsilon 0.8146391699382505, time_taken 724.0\n",
      "episode 420, reward -406.0, memory_length 2000, epsilon 0.8105761401277274, time_taken 730.0\n",
      "episode 430, reward -65.0, memory_length 2000, epsilon 0.8065333747629252, time_taken 727.0\n",
      "episode 440, reward 110.0, memory_length 2000, epsilon 0.8025107727744989, time_taken 726.0\n",
      "episode 450, reward -140.0, memory_length 2000, epsilon 0.7985082335971895, time_taken 723.0\n",
      "episode 460, reward 36.0, memory_length 2000, epsilon 0.794525657167309, time_taken 728.0\n",
      "episode 470, reward 461.0, memory_length 2000, epsilon 0.7905629439202393, time_taken 727.0\n",
      "episode 480, reward -410.0, memory_length 2000, epsilon 0.7866199947879429, time_taken 729.0\n",
      "episode 490, reward 324.0, memory_length 2000, epsilon 0.7826967111964858, time_taken 724.0\n",
      "episode 500, reward -135.0, memory_length 2000, epsilon 0.7787929950635742, time_taken 730.0\n",
      "episode 510, reward -128.0, memory_length 2000, epsilon 0.7749087487961014, time_taken 725.0\n",
      "episode 520, reward 271.0, memory_length 2000, epsilon 0.7710438752877082, time_taken 726.0\n",
      "episode 530, reward -150.0, memory_length 2000, epsilon 0.767198277916356, time_taken 722.0\n",
      "episode 540, reward -318.0, memory_length 2000, epsilon 0.7633718605419099, time_taken 723.0\n",
      "episode 550, reward -194.0, memory_length 2000, epsilon 0.7595645275037363, time_taken 731.0\n",
      "episode 560, reward -388.0, memory_length 2000, epsilon 0.7557761836183109, time_taken 724.0\n",
      "episode 570, reward -163.0, memory_length 2000, epsilon 0.7520067341768395, time_taken 729.0\n",
      "episode 580, reward -266.0, memory_length 2000, epsilon 0.7482560849428895, time_taken 727.0\n",
      "episode 590, reward -563.0, memory_length 2000, epsilon 0.7445241421500348, time_taken 724.0\n",
      "episode 600, reward 52.0, memory_length 2000, epsilon 0.7408108124995111, time_taken 725.0\n",
      "episode 610, reward 244.0, memory_length 2000, epsilon 0.737116003157884, time_taken 737.0\n",
      "episode 620, reward -188.0, memory_length 2000, epsilon 0.733439621754727, time_taken 730.0\n",
      "episode 630, reward -414.0, memory_length 2000, epsilon 0.7297815763803142, time_taken 730.0\n",
      "episode 640, reward -734.0, memory_length 2000, epsilon 0.7261417755833203, time_taken 728.0\n",
      "episode 650, reward -361.0, memory_length 2000, epsilon 0.7225201283685359, time_taken 729.0\n",
      "episode 660, reward -363.0, memory_length 2000, epsilon 0.7189165441945918, time_taken 725.0\n",
      "episode 670, reward 371.0, memory_length 2000, epsilon 0.7153309329716964, time_taken 734.0\n",
      "episode 680, reward -47.0, memory_length 2000, epsilon 0.7117632050593821, time_taken 730.0\n",
      "episode 690, reward -226.0, memory_length 2000, epsilon 0.7082132712642654, time_taken 729.0\n",
      "episode 700, reward -34.0, memory_length 2000, epsilon 0.7046810428378163, time_taken 727.0\n",
      "episode 710, reward -280.0, memory_length 2000, epsilon 0.7011664314741404, time_taken 729.0\n",
      "episode 720, reward -245.0, memory_length 2000, epsilon 0.6976693493077704, time_taken 724.0\n",
      "episode 730, reward -415.0, memory_length 2000, epsilon 0.6941897089114701, time_taken 724.0\n",
      "episode 740, reward -25.0, memory_length 2000, epsilon 0.6907274232940483, time_taken 733.0\n",
      "episode 750, reward -186.0, memory_length 2000, epsilon 0.6872824058981843, time_taken 726.0\n",
      "episode 760, reward -184.0, memory_length 2000, epsilon 0.6838545705982637, time_taken 723.0\n",
      "episode 770, reward 387.0, memory_length 2000, epsilon 0.6804438316982256, time_taken 726.0\n",
      "episode 780, reward -193.0, memory_length 2000, epsilon 0.6770501039294197, time_taken 728.0\n",
      "episode 790, reward -8.0, memory_length 2000, epsilon 0.6736733024484752, time_taken 733.0\n",
      "episode 800, reward -197.0, memory_length 2000, epsilon 0.670313342835179, time_taken 726.0\n",
      "episode 810, reward 91.0, memory_length 2000, epsilon 0.6669701410903658, time_taken 723.0\n",
      "episode 820, reward -348.0, memory_length 2000, epsilon 0.6636436136338181, time_taken 728.0\n",
      "episode 830, reward 291.0, memory_length 2000, epsilon 0.6603336773021758, time_taken 730.0\n",
      "episode 840, reward -396.0, memory_length 2000, epsilon 0.6570402493468587, time_taken 729.0\n",
      "episode 850, reward -452.0, memory_length 2000, epsilon 0.653763247431996, time_taken 732.0\n",
      "episode 860, reward -916.0, memory_length 2000, epsilon 0.6505025896323693, time_taken 729.0\n",
      "episode 870, reward 173.0, memory_length 2000, epsilon 0.6472581944313639, time_taken 733.0\n",
      "episode 880, reward -647.0, memory_length 2000, epsilon 0.6440299807189306, time_taken 726.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 890, reward -380.0, memory_length 2000, epsilon 0.6408178677895584, time_taken 722.0\n",
      "episode 900, reward 48.0, memory_length 2000, epsilon 0.6376217753402571, time_taken 721.0\n",
      "episode 910, reward 313.0, memory_length 2000, epsilon 0.6344416234685488, time_taken 727.0\n",
      "episode 920, reward -171.0, memory_length 2000, epsilon 0.6312773326704709, time_taken 730.0\n",
      "episode 930, reward 105.0, memory_length 2000, epsilon 0.6281288238385889, time_taken 723.0\n",
      "episode 940, reward -61.0, memory_length 2000, epsilon 0.6249960182600179, time_taken 728.0\n",
      "episode 950, reward -308.0, memory_length 2000, epsilon 0.6218788376144554, time_taken 731.0\n",
      "episode 960, reward -123.0, memory_length 2000, epsilon 0.6187772039722228, time_taken 730.0\n",
      "episode 970, reward 277.0, memory_length 2000, epsilon 0.6156910397923175, time_taken 735.0\n",
      "episode 980, reward -390.0, memory_length 2000, epsilon 0.6126202679204743, time_taken 731.0\n",
      "episode 990, reward 65.0, memory_length 2000, epsilon 0.6095648115872363, time_taken 731.0\n",
      "episode 1000, reward -628.0, memory_length 2000, epsilon 0.6065245944060363, time_taken 729.0\n",
      "model saved\n",
      "episode 1010, reward 466.0, memory_length 2000, epsilon 0.6034995403712863, time_taken 729.0\n",
      "episode 1020, reward -238.0, memory_length 2000, epsilon 0.6004895738564778, time_taken 732.0\n",
      "episode 1030, reward -1070.0, memory_length 2000, epsilon 0.5974946196122913, time_taken 724.0\n",
      "episode 1040, reward -365.0, memory_length 2000, epsilon 0.5945146027647147, time_taken 733.0\n",
      "episode 1050, reward -893.0, memory_length 2000, epsilon 0.5915494488131715, time_taken 728.0\n",
      "episode 1060, reward -1270.0, memory_length 2000, epsilon 0.5885990836286584, time_taken 724.0\n",
      "episode 1070, reward -304.0, memory_length 2000, epsilon 0.5856634334518923, time_taken 725.0\n",
      "episode 1080, reward 319.0, memory_length 2000, epsilon 0.5827424248914659, time_taken 731.0\n",
      "episode 1090, reward -26.0, memory_length 2000, epsilon 0.579835984922013, time_taken 722.0\n",
      "episode 1100, reward 165.0, memory_length 2000, epsilon 0.5769440408823828, time_taken 727.0\n",
      "episode 1110, reward -427.0, memory_length 2000, epsilon 0.574066520473824, time_taken 731.0\n",
      "episode 1120, reward 393.0, memory_length 2000, epsilon 0.5712033517581764, time_taken 728.0\n",
      "episode 1130, reward -21.0, memory_length 2000, epsilon 0.5683544631560729, time_taken 726.0\n",
      "episode 1140, reward -572.0, memory_length 2000, epsilon 0.5655197834451501, time_taken 726.0\n",
      "episode 1150, reward 498.0, memory_length 2000, epsilon 0.5626992417582677, time_taken 733.0\n",
      "episode 1160, reward -10.0, memory_length 2000, epsilon 0.5598927675817365, time_taken 727.0\n",
      "episode 1170, reward -91.0, memory_length 2000, epsilon 0.5571002907535558, time_taken 725.0\n",
      "episode 1180, reward -25.0, memory_length 2000, epsilon 0.5543217414616598, time_taken 725.0\n",
      "episode 1190, reward 218.0, memory_length 2000, epsilon 0.5515570502421712, time_taken 722.0\n",
      "episode 1200, reward 248.0, memory_length 2000, epsilon 0.5488061479776656, time_taken 732.0\n",
      "episode 1210, reward -580.0, memory_length 2000, epsilon 0.5460689658954431, time_taken 725.0\n",
      "episode 1220, reward -295.0, memory_length 2000, epsilon 0.543345435565809, time_taken 726.0\n",
      "episode 1230, reward -90.0, memory_length 2000, epsilon 0.5406354889003635, time_taken 726.0\n",
      "episode 1240, reward -469.0, memory_length 2000, epsilon 0.5379390581502985, time_taken 728.0\n",
      "episode 1250, reward 199.0, memory_length 2000, epsilon 0.5352560759047051, time_taken 731.0\n",
      "episode 1260, reward -551.0, memory_length 2000, epsilon 0.5325864750888871, time_taken 723.0\n",
      "episode 1270, reward -989.0, memory_length 2000, epsilon 0.5299301889626854, time_taken 727.0\n",
      "episode 1280, reward -316.0, memory_length 2000, epsilon 0.5272871511188082, time_taken 726.0\n",
      "episode 1290, reward -812.0, memory_length 2000, epsilon 0.5246572954811719, time_taken 728.0\n",
      "episode 1300, reward -1027.0, memory_length 2000, epsilon 0.5220405563032484, time_taken 727.0\n",
      "episode 1310, reward -273.0, memory_length 2000, epsilon 0.5194368681664223, time_taken 727.0\n",
      "episode 1320, reward -957.0, memory_length 2000, epsilon 0.5168461659783543, time_taken 726.0\n",
      "episode 1330, reward -25.0, memory_length 2000, epsilon 0.5142683849713549, time_taken 728.0\n",
      "episode 1340, reward -201.0, memory_length 2000, epsilon 0.5117034607007646, time_taken 733.0\n",
      "episode 1350, reward -822.0, memory_length 2000, epsilon 0.5091513290433431, time_taken 728.0\n",
      "episode 1360, reward 569.0, memory_length 2000, epsilon 0.5066119261956659, time_taken 734.0\n",
      "episode 1370, reward -849.0, memory_length 2000, epsilon 0.5040851886725298, time_taken 727.0\n",
      "episode 1380, reward -570.0, memory_length 2000, epsilon 0.5015710533053648, time_taken 734.0\n",
      "episode 1390, reward 74.0, memory_length 2000, epsilon 0.4990694572406561, time_taken 724.0\n",
      "episode 1400, reward -463.0, memory_length 2000, epsilon 0.4965803379383716, time_taken 724.0\n",
      "episode 1410, reward 593.0, memory_length 2000, epsilon 0.4941036331703991, time_taken 727.0\n",
      "episode 1420, reward -132.0, memory_length 2000, epsilon 0.4916392810189905, time_taken 732.0\n",
      "episode 1430, reward -259.0, memory_length 2000, epsilon 0.4891872198752136, time_taken 727.0\n",
      "episode 1440, reward -156.0, memory_length 2000, epsilon 0.4867473884374121, time_taken 733.0\n",
      "episode 1450, reward -1069.0, memory_length 2000, epsilon 0.4843197257096729, time_taken 721.0\n",
      "episode 1460, reward -191.0, memory_length 2000, epsilon 0.4819041710003016, time_taken 733.0\n",
      "episode 1470, reward -660.0, memory_length 2000, epsilon 0.4795006639203044, time_taken 724.0\n",
      "episode 1480, reward -872.0, memory_length 2000, epsilon 0.4771091443818792, time_taken 729.0\n",
      "episode 1490, reward 1000.0, memory_length 2000, epsilon 0.474729552596913, time_taken 726.0\n",
      "episode 1500, reward -68.0, memory_length 2000, epsilon 0.4723618290754873, time_taken 732.0\n",
      "episode 1510, reward 105.0, memory_length 2000, epsilon 0.4700059146243907, time_taken 726.0\n",
      "episode 1520, reward -632.0, memory_length 2000, epsilon 0.46766175034563917, time_taken 724.0\n",
      "episode 1530, reward -655.0, memory_length 2000, epsilon 0.4653292776350037, time_taken 729.0\n",
      "episode 1540, reward 92.0, memory_length 2000, epsilon 0.463008438180545, time_taken 729.0\n",
      "episode 1550, reward -536.0, memory_length 2000, epsilon 0.46069917396115584, time_taken 729.0\n",
      "episode 1560, reward 203.0, memory_length 2000, epsilon 0.45840142724511046, time_taken 725.0\n",
      "episode 1570, reward 661.0, memory_length 2000, epsilon 0.45611514058862135, time_taken 725.0\n",
      "episode 1580, reward -495.0, memory_length 2000, epsilon 0.45384025683440304, time_taken 725.0\n",
      "episode 1590, reward -326.0, memory_length 2000, epsilon 0.451576719110243, time_taken 727.0\n",
      "episode 1600, reward 62.0, memory_length 2000, epsilon 0.4493244708275804, time_taken 726.0\n",
      "episode 1610, reward 58.0, memory_length 2000, epsilon 0.44708345568009084, time_taken 737.0\n",
      "episode 1620, reward -409.0, memory_length 2000, epsilon 0.4448536176422789, time_taken 727.0\n",
      "episode 1630, reward 430.0, memory_length 2000, epsilon 0.4426349009680775, time_taken 726.0\n",
      "episode 1640, reward -19.0, memory_length 2000, epsilon 0.4404272501894542, time_taken 730.0\n",
      "episode 1650, reward 169.0, memory_length 2000, epsilon 0.43823061011502457, time_taken 725.0\n",
      "episode 1660, reward -638.0, memory_length 2000, epsilon 0.4360449258286724, time_taken 723.0\n",
      "episode 1670, reward 253.0, memory_length 2000, epsilon 0.4338701426881766, time_taken 723.0\n",
      "episode 1680, reward 9.0, memory_length 2000, epsilon 0.43170620632384543, time_taken 732.0\n",
      "episode 1690, reward -280.0, memory_length 2000, epsilon 0.42955306263715703, time_taken 727.0\n",
      "episode 1700, reward -624.0, memory_length 2000, epsilon 0.42741065779940723, time_taken 724.0\n",
      "episode 1710, reward -24.0, memory_length 2000, epsilon 0.42527893825036334, time_taken 730.0\n",
      "episode 1720, reward -561.0, memory_length 2000, epsilon 0.4231578506969257, time_taken 727.0\n",
      "episode 1730, reward 134.0, memory_length 2000, epsilon 0.42104734211179495, time_taken 725.0\n",
      "episode 1740, reward -1463.0, memory_length 2000, epsilon 0.41894735973214653, time_taken 730.0\n",
      "episode 1750, reward -831.0, memory_length 2000, epsilon 0.41685785105831163, time_taken 725.0\n",
      "episode 1760, reward 427.0, memory_length 2000, epsilon 0.4147787638524646, time_taken 730.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1770, reward -371.0, memory_length 2000, epsilon 0.41271004613731693, time_taken 728.0\n",
      "episode 1780, reward -1855.0, memory_length 2000, epsilon 0.410651646194818, time_taken 727.0\n",
      "episode 1790, reward -485.0, memory_length 2000, epsilon 0.40860351256486205, time_taken 728.0\n",
      "episode 1800, reward 527.0, memory_length 2000, epsilon 0.4065655940440017, time_taken 731.0\n",
      "episode 1810, reward 274.0, memory_length 2000, epsilon 0.4045378396841678, time_taken 725.0\n",
      "episode 1820, reward 562.0, memory_length 2000, epsilon 0.40252019879139567, time_taken 723.0\n",
      "episode 1830, reward -162.0, memory_length 2000, epsilon 0.4005126209245579, time_taken 729.0\n",
      "episode 1840, reward -385.0, memory_length 2000, epsilon 0.3985150558941033, time_taken 723.0\n",
      "episode 1850, reward 632.0, memory_length 2000, epsilon 0.3965274537608021, time_taken 723.0\n",
      "episode 1860, reward -726.0, memory_length 2000, epsilon 0.3945497648344974, time_taken 723.0\n",
      "episode 1870, reward -182.0, memory_length 2000, epsilon 0.3925819396728631, time_taken 728.0\n",
      "episode 1880, reward -203.0, memory_length 2000, epsilon 0.3906239290801675, time_taken 728.0\n",
      "episode 1890, reward -812.0, memory_length 2000, epsilon 0.388675684106044, time_taken 732.0\n",
      "episode 1900, reward -573.0, memory_length 2000, epsilon 0.3867371560442667, time_taken 731.0\n",
      "episode 1910, reward -359.0, memory_length 2000, epsilon 0.384808296431533, time_taken 731.0\n",
      "episode 1920, reward 416.0, memory_length 2000, epsilon 0.3828890570462523, time_taken 727.0\n",
      "episode 1930, reward -186.0, memory_length 2000, epsilon 0.38097938990733987, time_taken 729.0\n",
      "episode 1940, reward 158.0, memory_length 2000, epsilon 0.37907924727301784, time_taken 738.0\n",
      "episode 1950, reward -52.0, memory_length 2000, epsilon 0.3771885816396213, time_taken 740.0\n",
      "episode 1960, reward -828.0, memory_length 2000, epsilon 0.3753073457404111, time_taken 726.0\n",
      "episode 1970, reward 54.0, memory_length 2000, epsilon 0.37343549254439157, time_taken 725.0\n",
      "episode 1980, reward -406.0, memory_length 2000, epsilon 0.3715729752551355, time_taken 724.0\n",
      "episode 1990, reward -1448.0, memory_length 2000, epsilon 0.36971974730961354, time_taken 727.0\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "#call Environment class\n",
    "env = CabDriver()\n",
    "\n",
    "#get state and action size\n",
    "state_size = 5+7+24+5+5\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "agent= DQNAgent(state_size,action_size)\n",
    "\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "episode_time = 24*30 #a month value in hours\n",
    "LR = agent.learning_rate\n",
    "GAMMA = agent.discount_factor\n",
    "threshold = 200\n",
    "policy_threshold = 30000\n",
    "if not os.path.exists(\"saved_model_weights\"):\n",
    "    os.mkdir(\"saved_model_weights\")\n",
    "    \n",
    "for episode in range(Episodes):\n",
    "    terminal_state=False\n",
    "    score = 0\n",
    "    time_stamp=0\n",
    "    total_time = 0\n",
    "    time_state_to_end = 0\n",
    "    env = CabDriver()\n",
    "    action_space,state_space,state = env.reset()\n",
    "    #agent.epsilon = - 1/ (1 + np.exp((-episode+7500000)/17000000)) + 1\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "    \n",
    "    initial_state = env.state_init\n",
    "    \n",
    "    while not terminal_state:\n",
    "        if time_stamp > episode_time:\n",
    "            terminal_state=True;\n",
    "        z = np.random.random()\n",
    "        #time_stamp+=1\n",
    "        \n",
    "        action = env.requests(state)[1]\n",
    "        agent.action_size=len(action)\n",
    "        take_action=agent.get_action(env.state_encod_arch2(state,(0,0)),action)\n",
    "        \n",
    "        next_state,time_state_to_end = env.next_state_func(state,take_action,Time_matrix)\n",
    "        reward= env.reward_func(state,take_action,Time_matrix)\n",
    "#         reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "        time_stamp += time_state_to_end\n",
    "        \n",
    "        agent.append_sample(state, take_action, reward, next_state)\n",
    "        max_next = max(Q_dict[next_state],key=Q_dict[next_state].get)\n",
    "        Q_dict[state][take_action] += LR * ((reward + (GAMMA*(Q_dict[next_state][max_next]))) - Q_dict[state][take_action] ) \n",
    "            \n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        agent.train_model(terminal_state)\n",
    "        \n",
    "    \n",
    "        \n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "#     if agent.epsilon > agent.epsilon_min:\n",
    "#         agent.epsilon *= agent.epsilon_decay\n",
    "    \n",
    "    if (episode % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time_taken {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon,time_stamp))\n",
    "    if episode % 1000 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        agent.save(name=\"model_weights.h5\")\n",
    "        print(\"model saved\")\n",
    "\n",
    "    if initial_state in rewards_tracked:     #storing rewards\n",
    "        rewards_tracked[initial_state].append(score)\n",
    "        #save_obj(rewards_tracked,'Rewards')\n",
    "\n",
    "    if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "        save_obj(rewards_tracked,'Rewards')   \n",
    "    \n",
    "#     #TRACKING Q-VALUES\n",
    "#     if (episode == threshold-1):        #at the 1999th episode\n",
    "#         initialise_tracking_states()\n",
    "      \n",
    "#     if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "#         save_tracking_states()\n",
    "#         save_obj(States_track,'States_tracked')   \n",
    "    \n",
    "    #SAVING POLICY\n",
    "    if ((episode+1)% policy_threshold ) == 0:  #every 30000th episodes, the Q-dict will be saved\n",
    "        save_obj(Q_dict,'Policy')    \n",
    "        \n",
    "        \n",
    "save_obj(rewards_tracked,'Rewards')   \n",
    "save_obj(States_track,'States_tracked')   \n",
    "save_obj(Q_dict,'Policy')      \n",
    "print(episode)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in env.state_space:\n",
    "    if arr[2]==25:\n",
    "        print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (0, 0, 2),\n",
       " (0, 0, 3),\n",
       " (0, 0, 4),\n",
       " (0, 0, 5),\n",
       " (0, 0, 6),\n",
       " (0, 1, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 2),\n",
       " (0, 1, 3),\n",
       " (0, 1, 4),\n",
       " (0, 1, 5),\n",
       " (0, 1, 6),\n",
       " (0, 2, 0),\n",
       " (0, 2, 1),\n",
       " (0, 2, 2),\n",
       " (0, 2, 3),\n",
       " (0, 2, 4),\n",
       " (0, 2, 5),\n",
       " (0, 2, 6),\n",
       " (0, 3, 0),\n",
       " (0, 3, 1),\n",
       " (0, 3, 2),\n",
       " (0, 3, 3),\n",
       " (0, 3, 4),\n",
       " (0, 3, 5),\n",
       " (0, 3, 6),\n",
       " (0, 4, 0),\n",
       " (0, 4, 1),\n",
       " (0, 4, 2),\n",
       " (0, 4, 3),\n",
       " (0, 4, 4),\n",
       " (0, 4, 5),\n",
       " (0, 4, 6),\n",
       " (0, 5, 0),\n",
       " (0, 5, 1),\n",
       " (0, 5, 2),\n",
       " (0, 5, 3),\n",
       " (0, 5, 4),\n",
       " (0, 5, 5),\n",
       " (0, 5, 6),\n",
       " (0, 6, 0),\n",
       " (0, 6, 1),\n",
       " (0, 6, 2),\n",
       " (0, 6, 3),\n",
       " (0, 6, 4),\n",
       " (0, 6, 5),\n",
       " (0, 6, 6),\n",
       " (0, 7, 0),\n",
       " (0, 7, 1),\n",
       " (0, 7, 2),\n",
       " (0, 7, 3),\n",
       " (0, 7, 4),\n",
       " (0, 7, 5),\n",
       " (0, 7, 6),\n",
       " (0, 8, 0),\n",
       " (0, 8, 1),\n",
       " (0, 8, 2),\n",
       " (0, 8, 3),\n",
       " (0, 8, 4),\n",
       " (0, 8, 5),\n",
       " (0, 8, 6),\n",
       " (0, 9, 0),\n",
       " (0, 9, 1),\n",
       " (0, 9, 2),\n",
       " (0, 9, 3),\n",
       " (0, 9, 4),\n",
       " (0, 9, 5),\n",
       " (0, 9, 6),\n",
       " (0, 10, 0),\n",
       " (0, 10, 1),\n",
       " (0, 10, 2),\n",
       " (0, 10, 3),\n",
       " (0, 10, 4),\n",
       " (0, 10, 5),\n",
       " (0, 10, 6),\n",
       " (0, 11, 0),\n",
       " (0, 11, 1),\n",
       " (0, 11, 2),\n",
       " (0, 11, 3),\n",
       " (0, 11, 4),\n",
       " (0, 11, 5),\n",
       " (0, 11, 6),\n",
       " (0, 12, 0),\n",
       " (0, 12, 1),\n",
       " (0, 12, 2),\n",
       " (0, 12, 3),\n",
       " (0, 12, 4),\n",
       " (0, 12, 5),\n",
       " (0, 12, 6),\n",
       " (0, 13, 0),\n",
       " (0, 13, 1),\n",
       " (0, 13, 2),\n",
       " (0, 13, 3),\n",
       " (0, 13, 4),\n",
       " (0, 13, 5),\n",
       " (0, 13, 6),\n",
       " (0, 14, 0),\n",
       " (0, 14, 1),\n",
       " (0, 14, 2),\n",
       " (0, 14, 3),\n",
       " (0, 14, 4),\n",
       " (0, 14, 5),\n",
       " (0, 14, 6),\n",
       " (0, 15, 0),\n",
       " (0, 15, 1),\n",
       " (0, 15, 2),\n",
       " (0, 15, 3),\n",
       " (0, 15, 4),\n",
       " (0, 15, 5),\n",
       " (0, 15, 6),\n",
       " (0, 16, 0),\n",
       " (0, 16, 1),\n",
       " (0, 16, 2),\n",
       " (0, 16, 3),\n",
       " (0, 16, 4),\n",
       " (0, 16, 5),\n",
       " (0, 16, 6),\n",
       " (0, 17, 0),\n",
       " (0, 17, 1),\n",
       " (0, 17, 2),\n",
       " (0, 17, 3),\n",
       " (0, 17, 4),\n",
       " (0, 17, 5),\n",
       " (0, 17, 6),\n",
       " (0, 18, 0),\n",
       " (0, 18, 1),\n",
       " (0, 18, 2),\n",
       " (0, 18, 3),\n",
       " (0, 18, 4),\n",
       " (0, 18, 5),\n",
       " (0, 18, 6),\n",
       " (0, 19, 0),\n",
       " (0, 19, 1),\n",
       " (0, 19, 2),\n",
       " (0, 19, 3),\n",
       " (0, 19, 4),\n",
       " (0, 19, 5),\n",
       " (0, 19, 6),\n",
       " (0, 20, 0),\n",
       " (0, 20, 1),\n",
       " (0, 20, 2),\n",
       " (0, 20, 3),\n",
       " (0, 20, 4),\n",
       " (0, 20, 5),\n",
       " (0, 20, 6),\n",
       " (0, 21, 0),\n",
       " (0, 21, 1),\n",
       " (0, 21, 2),\n",
       " (0, 21, 3),\n",
       " (0, 21, 4),\n",
       " (0, 21, 5),\n",
       " (0, 21, 6),\n",
       " (0, 22, 0),\n",
       " (0, 22, 1),\n",
       " (0, 22, 2),\n",
       " (0, 22, 3),\n",
       " (0, 22, 4),\n",
       " (0, 22, 5),\n",
       " (0, 22, 6),\n",
       " (0, 23, 0),\n",
       " (0, 23, 1),\n",
       " (0, 23, 2),\n",
       " (0, 23, 3),\n",
       " (0, 23, 4),\n",
       " (0, 23, 5),\n",
       " (0, 23, 6),\n",
       " (1, 0, 0),\n",
       " (1, 0, 1),\n",
       " (1, 0, 2),\n",
       " (1, 0, 3),\n",
       " (1, 0, 4),\n",
       " (1, 0, 5),\n",
       " (1, 0, 6),\n",
       " (1, 1, 0),\n",
       " (1, 1, 1),\n",
       " (1, 1, 2),\n",
       " (1, 1, 3),\n",
       " (1, 1, 4),\n",
       " (1, 1, 5),\n",
       " (1, 1, 6),\n",
       " (1, 2, 0),\n",
       " (1, 2, 1),\n",
       " (1, 2, 2),\n",
       " (1, 2, 3),\n",
       " (1, 2, 4),\n",
       " (1, 2, 5),\n",
       " (1, 2, 6),\n",
       " (1, 3, 0),\n",
       " (1, 3, 1),\n",
       " (1, 3, 2),\n",
       " (1, 3, 3),\n",
       " (1, 3, 4),\n",
       " (1, 3, 5),\n",
       " (1, 3, 6),\n",
       " (1, 4, 0),\n",
       " (1, 4, 1),\n",
       " (1, 4, 2),\n",
       " (1, 4, 3),\n",
       " (1, 4, 4),\n",
       " (1, 4, 5),\n",
       " (1, 4, 6),\n",
       " (1, 5, 0),\n",
       " (1, 5, 1),\n",
       " (1, 5, 2),\n",
       " (1, 5, 3),\n",
       " (1, 5, 4),\n",
       " (1, 5, 5),\n",
       " (1, 5, 6),\n",
       " (1, 6, 0),\n",
       " (1, 6, 1),\n",
       " (1, 6, 2),\n",
       " (1, 6, 3),\n",
       " (1, 6, 4),\n",
       " (1, 6, 5),\n",
       " (1, 6, 6),\n",
       " (1, 7, 0),\n",
       " (1, 7, 1),\n",
       " (1, 7, 2),\n",
       " (1, 7, 3),\n",
       " (1, 7, 4),\n",
       " (1, 7, 5),\n",
       " (1, 7, 6),\n",
       " (1, 8, 0),\n",
       " (1, 8, 1),\n",
       " (1, 8, 2),\n",
       " (1, 8, 3),\n",
       " (1, 8, 4),\n",
       " (1, 8, 5),\n",
       " (1, 8, 6),\n",
       " (1, 9, 0),\n",
       " (1, 9, 1),\n",
       " (1, 9, 2),\n",
       " (1, 9, 3),\n",
       " (1, 9, 4),\n",
       " (1, 9, 5),\n",
       " (1, 9, 6),\n",
       " (1, 10, 0),\n",
       " (1, 10, 1),\n",
       " (1, 10, 2),\n",
       " (1, 10, 3),\n",
       " (1, 10, 4),\n",
       " (1, 10, 5),\n",
       " (1, 10, 6),\n",
       " (1, 11, 0),\n",
       " (1, 11, 1),\n",
       " (1, 11, 2),\n",
       " (1, 11, 3),\n",
       " (1, 11, 4),\n",
       " (1, 11, 5),\n",
       " (1, 11, 6),\n",
       " (1, 12, 0),\n",
       " (1, 12, 1),\n",
       " (1, 12, 2),\n",
       " (1, 12, 3),\n",
       " (1, 12, 4),\n",
       " (1, 12, 5),\n",
       " (1, 12, 6),\n",
       " (1, 13, 0),\n",
       " (1, 13, 1),\n",
       " (1, 13, 2),\n",
       " (1, 13, 3),\n",
       " (1, 13, 4),\n",
       " (1, 13, 5),\n",
       " (1, 13, 6),\n",
       " (1, 14, 0),\n",
       " (1, 14, 1),\n",
       " (1, 14, 2),\n",
       " (1, 14, 3),\n",
       " (1, 14, 4),\n",
       " (1, 14, 5),\n",
       " (1, 14, 6),\n",
       " (1, 15, 0),\n",
       " (1, 15, 1),\n",
       " (1, 15, 2),\n",
       " (1, 15, 3),\n",
       " (1, 15, 4),\n",
       " (1, 15, 5),\n",
       " (1, 15, 6),\n",
       " (1, 16, 0),\n",
       " (1, 16, 1),\n",
       " (1, 16, 2),\n",
       " (1, 16, 3),\n",
       " (1, 16, 4),\n",
       " (1, 16, 5),\n",
       " (1, 16, 6),\n",
       " (1, 17, 0),\n",
       " (1, 17, 1),\n",
       " (1, 17, 2),\n",
       " (1, 17, 3),\n",
       " (1, 17, 4),\n",
       " (1, 17, 5),\n",
       " (1, 17, 6),\n",
       " (1, 18, 0),\n",
       " (1, 18, 1),\n",
       " (1, 18, 2),\n",
       " (1, 18, 3),\n",
       " (1, 18, 4),\n",
       " (1, 18, 5),\n",
       " (1, 18, 6),\n",
       " (1, 19, 0),\n",
       " (1, 19, 1),\n",
       " (1, 19, 2),\n",
       " (1, 19, 3),\n",
       " (1, 19, 4),\n",
       " (1, 19, 5),\n",
       " (1, 19, 6),\n",
       " (1, 20, 0),\n",
       " (1, 20, 1),\n",
       " (1, 20, 2),\n",
       " (1, 20, 3),\n",
       " (1, 20, 4),\n",
       " (1, 20, 5),\n",
       " (1, 20, 6),\n",
       " (1, 21, 0),\n",
       " (1, 21, 1),\n",
       " (1, 21, 2),\n",
       " (1, 21, 3),\n",
       " (1, 21, 4),\n",
       " (1, 21, 5),\n",
       " (1, 21, 6),\n",
       " (1, 22, 0),\n",
       " (1, 22, 1),\n",
       " (1, 22, 2),\n",
       " (1, 22, 3),\n",
       " (1, 22, 4),\n",
       " (1, 22, 5),\n",
       " (1, 22, 6),\n",
       " (1, 23, 0),\n",
       " (1, 23, 1),\n",
       " (1, 23, 2),\n",
       " (1, 23, 3),\n",
       " (1, 23, 4),\n",
       " (1, 23, 5),\n",
       " (1, 23, 6),\n",
       " (2, 0, 0),\n",
       " (2, 0, 1),\n",
       " (2, 0, 2),\n",
       " (2, 0, 3),\n",
       " (2, 0, 4),\n",
       " (2, 0, 5),\n",
       " (2, 0, 6),\n",
       " (2, 1, 0),\n",
       " (2, 1, 1),\n",
       " (2, 1, 2),\n",
       " (2, 1, 3),\n",
       " (2, 1, 4),\n",
       " (2, 1, 5),\n",
       " (2, 1, 6),\n",
       " (2, 2, 0),\n",
       " (2, 2, 1),\n",
       " (2, 2, 2),\n",
       " (2, 2, 3),\n",
       " (2, 2, 4),\n",
       " (2, 2, 5),\n",
       " (2, 2, 6),\n",
       " (2, 3, 0),\n",
       " (2, 3, 1),\n",
       " (2, 3, 2),\n",
       " (2, 3, 3),\n",
       " (2, 3, 4),\n",
       " (2, 3, 5),\n",
       " (2, 3, 6),\n",
       " (2, 4, 0),\n",
       " (2, 4, 1),\n",
       " (2, 4, 2),\n",
       " (2, 4, 3),\n",
       " (2, 4, 4),\n",
       " (2, 4, 5),\n",
       " (2, 4, 6),\n",
       " (2, 5, 0),\n",
       " (2, 5, 1),\n",
       " (2, 5, 2),\n",
       " (2, 5, 3),\n",
       " (2, 5, 4),\n",
       " (2, 5, 5),\n",
       " (2, 5, 6),\n",
       " (2, 6, 0),\n",
       " (2, 6, 1),\n",
       " (2, 6, 2),\n",
       " (2, 6, 3),\n",
       " (2, 6, 4),\n",
       " (2, 6, 5),\n",
       " (2, 6, 6),\n",
       " (2, 7, 0),\n",
       " (2, 7, 1),\n",
       " (2, 7, 2),\n",
       " (2, 7, 3),\n",
       " (2, 7, 4),\n",
       " (2, 7, 5),\n",
       " (2, 7, 6),\n",
       " (2, 8, 0),\n",
       " (2, 8, 1),\n",
       " (2, 8, 2),\n",
       " (2, 8, 3),\n",
       " (2, 8, 4),\n",
       " (2, 8, 5),\n",
       " (2, 8, 6),\n",
       " (2, 9, 0),\n",
       " (2, 9, 1),\n",
       " (2, 9, 2),\n",
       " (2, 9, 3),\n",
       " (2, 9, 4),\n",
       " (2, 9, 5),\n",
       " (2, 9, 6),\n",
       " (2, 10, 0),\n",
       " (2, 10, 1),\n",
       " (2, 10, 2),\n",
       " (2, 10, 3),\n",
       " (2, 10, 4),\n",
       " (2, 10, 5),\n",
       " (2, 10, 6),\n",
       " (2, 11, 0),\n",
       " (2, 11, 1),\n",
       " (2, 11, 2),\n",
       " (2, 11, 3),\n",
       " (2, 11, 4),\n",
       " (2, 11, 5),\n",
       " (2, 11, 6),\n",
       " (2, 12, 0),\n",
       " (2, 12, 1),\n",
       " (2, 12, 2),\n",
       " (2, 12, 3),\n",
       " (2, 12, 4),\n",
       " (2, 12, 5),\n",
       " (2, 12, 6),\n",
       " (2, 13, 0),\n",
       " (2, 13, 1),\n",
       " (2, 13, 2),\n",
       " (2, 13, 3),\n",
       " (2, 13, 4),\n",
       " (2, 13, 5),\n",
       " (2, 13, 6),\n",
       " (2, 14, 0),\n",
       " (2, 14, 1),\n",
       " (2, 14, 2),\n",
       " (2, 14, 3),\n",
       " (2, 14, 4),\n",
       " (2, 14, 5),\n",
       " (2, 14, 6),\n",
       " (2, 15, 0),\n",
       " (2, 15, 1),\n",
       " (2, 15, 2),\n",
       " (2, 15, 3),\n",
       " (2, 15, 4),\n",
       " (2, 15, 5),\n",
       " (2, 15, 6),\n",
       " (2, 16, 0),\n",
       " (2, 16, 1),\n",
       " (2, 16, 2),\n",
       " (2, 16, 3),\n",
       " (2, 16, 4),\n",
       " (2, 16, 5),\n",
       " (2, 16, 6),\n",
       " (2, 17, 0),\n",
       " (2, 17, 1),\n",
       " (2, 17, 2),\n",
       " (2, 17, 3),\n",
       " (2, 17, 4),\n",
       " (2, 17, 5),\n",
       " (2, 17, 6),\n",
       " (2, 18, 0),\n",
       " (2, 18, 1),\n",
       " (2, 18, 2),\n",
       " (2, 18, 3),\n",
       " (2, 18, 4),\n",
       " (2, 18, 5),\n",
       " (2, 18, 6),\n",
       " (2, 19, 0),\n",
       " (2, 19, 1),\n",
       " (2, 19, 2),\n",
       " (2, 19, 3),\n",
       " (2, 19, 4),\n",
       " (2, 19, 5),\n",
       " (2, 19, 6),\n",
       " (2, 20, 0),\n",
       " (2, 20, 1),\n",
       " (2, 20, 2),\n",
       " (2, 20, 3),\n",
       " (2, 20, 4),\n",
       " (2, 20, 5),\n",
       " (2, 20, 6),\n",
       " (2, 21, 0),\n",
       " (2, 21, 1),\n",
       " (2, 21, 2),\n",
       " (2, 21, 3),\n",
       " (2, 21, 4),\n",
       " (2, 21, 5),\n",
       " (2, 21, 6),\n",
       " (2, 22, 0),\n",
       " (2, 22, 1),\n",
       " (2, 22, 2),\n",
       " (2, 22, 3),\n",
       " (2, 22, 4),\n",
       " (2, 22, 5),\n",
       " (2, 22, 6),\n",
       " (2, 23, 0),\n",
       " (2, 23, 1),\n",
       " (2, 23, 2),\n",
       " (2, 23, 3),\n",
       " (2, 23, 4),\n",
       " (2, 23, 5),\n",
       " (2, 23, 6),\n",
       " (3, 0, 0),\n",
       " (3, 0, 1),\n",
       " (3, 0, 2),\n",
       " (3, 0, 3),\n",
       " (3, 0, 4),\n",
       " (3, 0, 5),\n",
       " (3, 0, 6),\n",
       " (3, 1, 0),\n",
       " (3, 1, 1),\n",
       " (3, 1, 2),\n",
       " (3, 1, 3),\n",
       " (3, 1, 4),\n",
       " (3, 1, 5),\n",
       " (3, 1, 6),\n",
       " (3, 2, 0),\n",
       " (3, 2, 1),\n",
       " (3, 2, 2),\n",
       " (3, 2, 3),\n",
       " (3, 2, 4),\n",
       " (3, 2, 5),\n",
       " (3, 2, 6),\n",
       " (3, 3, 0),\n",
       " (3, 3, 1),\n",
       " (3, 3, 2),\n",
       " (3, 3, 3),\n",
       " (3, 3, 4),\n",
       " (3, 3, 5),\n",
       " (3, 3, 6),\n",
       " (3, 4, 0),\n",
       " (3, 4, 1),\n",
       " (3, 4, 2),\n",
       " (3, 4, 3),\n",
       " (3, 4, 4),\n",
       " (3, 4, 5),\n",
       " (3, 4, 6),\n",
       " (3, 5, 0),\n",
       " (3, 5, 1),\n",
       " (3, 5, 2),\n",
       " (3, 5, 3),\n",
       " (3, 5, 4),\n",
       " (3, 5, 5),\n",
       " (3, 5, 6),\n",
       " (3, 6, 0),\n",
       " (3, 6, 1),\n",
       " (3, 6, 2),\n",
       " (3, 6, 3),\n",
       " (3, 6, 4),\n",
       " (3, 6, 5),\n",
       " (3, 6, 6),\n",
       " (3, 7, 0),\n",
       " (3, 7, 1),\n",
       " (3, 7, 2),\n",
       " (3, 7, 3),\n",
       " (3, 7, 4),\n",
       " (3, 7, 5),\n",
       " (3, 7, 6),\n",
       " (3, 8, 0),\n",
       " (3, 8, 1),\n",
       " (3, 8, 2),\n",
       " (3, 8, 3),\n",
       " (3, 8, 4),\n",
       " (3, 8, 5),\n",
       " (3, 8, 6),\n",
       " (3, 9, 0),\n",
       " (3, 9, 1),\n",
       " (3, 9, 2),\n",
       " (3, 9, 3),\n",
       " (3, 9, 4),\n",
       " (3, 9, 5),\n",
       " (3, 9, 6),\n",
       " (3, 10, 0),\n",
       " (3, 10, 1),\n",
       " (3, 10, 2),\n",
       " (3, 10, 3),\n",
       " (3, 10, 4),\n",
       " (3, 10, 5),\n",
       " (3, 10, 6),\n",
       " (3, 11, 0),\n",
       " (3, 11, 1),\n",
       " (3, 11, 2),\n",
       " (3, 11, 3),\n",
       " (3, 11, 4),\n",
       " (3, 11, 5),\n",
       " (3, 11, 6),\n",
       " (3, 12, 0),\n",
       " (3, 12, 1),\n",
       " (3, 12, 2),\n",
       " (3, 12, 3),\n",
       " (3, 12, 4),\n",
       " (3, 12, 5),\n",
       " (3, 12, 6),\n",
       " (3, 13, 0),\n",
       " (3, 13, 1),\n",
       " (3, 13, 2),\n",
       " (3, 13, 3),\n",
       " (3, 13, 4),\n",
       " (3, 13, 5),\n",
       " (3, 13, 6),\n",
       " (3, 14, 0),\n",
       " (3, 14, 1),\n",
       " (3, 14, 2),\n",
       " (3, 14, 3),\n",
       " (3, 14, 4),\n",
       " (3, 14, 5),\n",
       " (3, 14, 6),\n",
       " (3, 15, 0),\n",
       " (3, 15, 1),\n",
       " (3, 15, 2),\n",
       " (3, 15, 3),\n",
       " (3, 15, 4),\n",
       " (3, 15, 5),\n",
       " (3, 15, 6),\n",
       " (3, 16, 0),\n",
       " (3, 16, 1),\n",
       " (3, 16, 2),\n",
       " (3, 16, 3),\n",
       " (3, 16, 4),\n",
       " (3, 16, 5),\n",
       " (3, 16, 6),\n",
       " (3, 17, 0),\n",
       " (3, 17, 1),\n",
       " (3, 17, 2),\n",
       " (3, 17, 3),\n",
       " (3, 17, 4),\n",
       " (3, 17, 5),\n",
       " (3, 17, 6),\n",
       " (3, 18, 0),\n",
       " (3, 18, 1),\n",
       " (3, 18, 2),\n",
       " (3, 18, 3),\n",
       " (3, 18, 4),\n",
       " (3, 18, 5),\n",
       " (3, 18, 6),\n",
       " (3, 19, 0),\n",
       " (3, 19, 1),\n",
       " (3, 19, 2),\n",
       " (3, 19, 3),\n",
       " (3, 19, 4),\n",
       " (3, 19, 5),\n",
       " (3, 19, 6),\n",
       " (3, 20, 0),\n",
       " (3, 20, 1),\n",
       " (3, 20, 2),\n",
       " (3, 20, 3),\n",
       " (3, 20, 4),\n",
       " (3, 20, 5),\n",
       " (3, 20, 6),\n",
       " (3, 21, 0),\n",
       " (3, 21, 1),\n",
       " (3, 21, 2),\n",
       " (3, 21, 3),\n",
       " (3, 21, 4),\n",
       " (3, 21, 5),\n",
       " (3, 21, 6),\n",
       " (3, 22, 0),\n",
       " (3, 22, 1),\n",
       " (3, 22, 2),\n",
       " (3, 22, 3),\n",
       " (3, 22, 4),\n",
       " (3, 22, 5),\n",
       " (3, 22, 6),\n",
       " (3, 23, 0),\n",
       " (3, 23, 1),\n",
       " (3, 23, 2),\n",
       " (3, 23, 3),\n",
       " (3, 23, 4),\n",
       " (3, 23, 5),\n",
       " (3, 23, 6),\n",
       " (4, 0, 0),\n",
       " (4, 0, 1),\n",
       " (4, 0, 2),\n",
       " (4, 0, 3),\n",
       " (4, 0, 4),\n",
       " (4, 0, 5),\n",
       " (4, 0, 6),\n",
       " (4, 1, 0),\n",
       " (4, 1, 1),\n",
       " (4, 1, 2),\n",
       " (4, 1, 3),\n",
       " (4, 1, 4),\n",
       " (4, 1, 5),\n",
       " (4, 1, 6),\n",
       " (4, 2, 0),\n",
       " (4, 2, 1),\n",
       " (4, 2, 2),\n",
       " (4, 2, 3),\n",
       " (4, 2, 4),\n",
       " (4, 2, 5),\n",
       " (4, 2, 6),\n",
       " (4, 3, 0),\n",
       " (4, 3, 1),\n",
       " (4, 3, 2),\n",
       " (4, 3, 3),\n",
       " (4, 3, 4),\n",
       " (4, 3, 5),\n",
       " (4, 3, 6),\n",
       " (4, 4, 0),\n",
       " (4, 4, 1),\n",
       " (4, 4, 2),\n",
       " (4, 4, 3),\n",
       " (4, 4, 4),\n",
       " (4, 4, 5),\n",
       " (4, 4, 6),\n",
       " (4, 5, 0),\n",
       " (4, 5, 1),\n",
       " (4, 5, 2),\n",
       " (4, 5, 3),\n",
       " (4, 5, 4),\n",
       " (4, 5, 5),\n",
       " (4, 5, 6),\n",
       " (4, 6, 0),\n",
       " (4, 6, 1),\n",
       " (4, 6, 2),\n",
       " (4, 6, 3),\n",
       " (4, 6, 4),\n",
       " (4, 6, 5),\n",
       " (4, 6, 6),\n",
       " (4, 7, 0),\n",
       " (4, 7, 1),\n",
       " (4, 7, 2),\n",
       " (4, 7, 3),\n",
       " (4, 7, 4),\n",
       " (4, 7, 5),\n",
       " (4, 7, 6),\n",
       " (4, 8, 0),\n",
       " (4, 8, 1),\n",
       " (4, 8, 2),\n",
       " (4, 8, 3),\n",
       " (4, 8, 4),\n",
       " (4, 8, 5),\n",
       " (4, 8, 6),\n",
       " (4, 9, 0),\n",
       " (4, 9, 1),\n",
       " (4, 9, 2),\n",
       " (4, 9, 3),\n",
       " (4, 9, 4),\n",
       " (4, 9, 5),\n",
       " (4, 9, 6),\n",
       " (4, 10, 0),\n",
       " (4, 10, 1),\n",
       " (4, 10, 2),\n",
       " (4, 10, 3),\n",
       " (4, 10, 4),\n",
       " (4, 10, 5),\n",
       " (4, 10, 6),\n",
       " (4, 11, 0),\n",
       " (4, 11, 1),\n",
       " (4, 11, 2),\n",
       " (4, 11, 3),\n",
       " (4, 11, 4),\n",
       " (4, 11, 5),\n",
       " (4, 11, 6),\n",
       " (4, 12, 0),\n",
       " (4, 12, 1),\n",
       " (4, 12, 2),\n",
       " (4, 12, 3),\n",
       " (4, 12, 4),\n",
       " (4, 12, 5),\n",
       " (4, 12, 6),\n",
       " (4, 13, 0),\n",
       " (4, 13, 1),\n",
       " (4, 13, 2),\n",
       " (4, 13, 3),\n",
       " (4, 13, 4),\n",
       " (4, 13, 5),\n",
       " (4, 13, 6),\n",
       " (4, 14, 0),\n",
       " (4, 14, 1),\n",
       " (4, 14, 2),\n",
       " (4, 14, 3),\n",
       " (4, 14, 4),\n",
       " (4, 14, 5),\n",
       " (4, 14, 6),\n",
       " (4, 15, 0),\n",
       " (4, 15, 1),\n",
       " (4, 15, 2),\n",
       " (4, 15, 3),\n",
       " (4, 15, 4),\n",
       " (4, 15, 5),\n",
       " (4, 15, 6),\n",
       " (4, 16, 0),\n",
       " (4, 16, 1),\n",
       " (4, 16, 2),\n",
       " (4, 16, 3),\n",
       " (4, 16, 4),\n",
       " (4, 16, 5),\n",
       " (4, 16, 6),\n",
       " (4, 17, 0),\n",
       " (4, 17, 1),\n",
       " (4, 17, 2),\n",
       " (4, 17, 3),\n",
       " (4, 17, 4),\n",
       " (4, 17, 5),\n",
       " (4, 17, 6),\n",
       " (4, 18, 0),\n",
       " (4, 18, 1),\n",
       " (4, 18, 2),\n",
       " (4, 18, 3),\n",
       " (4, 18, 4),\n",
       " (4, 18, 5),\n",
       " (4, 18, 6),\n",
       " (4, 19, 0),\n",
       " (4, 19, 1),\n",
       " (4, 19, 2),\n",
       " (4, 19, 3),\n",
       " (4, 19, 4),\n",
       " (4, 19, 5),\n",
       " (4, 19, 6),\n",
       " (4, 20, 0),\n",
       " (4, 20, 1),\n",
       " (4, 20, 2),\n",
       " (4, 20, 3),\n",
       " (4, 20, 4),\n",
       " (4, 20, 5),\n",
       " (4, 20, 6),\n",
       " (4, 21, 0),\n",
       " (4, 21, 1),\n",
       " (4, 21, 2),\n",
       " (4, 21, 3),\n",
       " (4, 21, 4),\n",
       " (4, 21, 5),\n",
       " (4, 21, 6),\n",
       " (4, 22, 0),\n",
       " (4, 22, 1),\n",
       " (4, 22, 2),\n",
       " (4, 22, 3),\n",
       " (4, 22, 4),\n",
       " (4, 22, 5),\n",
       " (4, 22, 6),\n",
       " (4, 23, 0),\n",
       " (4, 23, 1),\n",
       " (4, 23, 2),\n",
       " (4, 23, 3),\n",
       " (4, 23, 4),\n",
       " (4, 23, 5),\n",
       " (4, 23, 6)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'states_tracked'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c63fa8d383cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstate_tracked_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStates_track\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_tracked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_tracked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'states_tracked'"
     ]
    }
   ],
   "source": [
    "state_tracked_sample = [agent.States_track[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "States_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CabDriver()\n",
    "state_size = env.state_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([10,14,15,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
